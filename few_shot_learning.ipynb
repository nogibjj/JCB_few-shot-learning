{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of Transformer-Based Few Shot Learning\n",
    "\n",
    "### ECE590 Homework assignment 6\n",
    "Name: Javier Cervantes\n",
    "\n",
    "net id: jc1010"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are interested in running experiments with Transformers applied to functional data. There are three papers we are focusing on, with associated code:\n",
    "\n",
    "• For the linear attention examples, we are focused on this paper: https://arxiv.org/abs/2212.07677 and the GitHub for the code is here: https://github.com/google-research/self-organising-systems/tree/master/transformers_learn_icl_by_gd\n",
    "\n",
    "• For the examples with softmax attention (traditional Transformer design), we are interested in this paper: https://arxiv.org/abs/2208.01066 and the GitHub for the code is here: https://github.com/dtsip/in-context-learning\n",
    "\n",
    "• The MAML GitHub may also be useful: https://github.com/cbfinn/maml . For this homework, it is ok to use/modify the above code to implement experiments.\n",
    "\n",
    "Consider the following process for generating contextual data for a linear model: weights $w_m \\in \\mathbb{R}^{10}$ are drawn for context $m$ as $w_m \\sim \\mathcal{N}(\\mu, I_{10})$ where $\\mu \\in \\mathbb{R}^{10}$ is a fixed mean vector. Covariates $x_i \\in \\mathbb{R}^{10}$ are drawn as $x_i \\sim \\mathcal{N}(0, I_{10})$. For contextual data $\\mathcal{C}_m$ draw one weight vector $w_m$ as above. For a context of length $N$ draw $x_{m,i}, i = 1, \\ldots, N$ as above, and for each $x_{m, i}$ constitute a corresponding $y_{m, i} = w_m^T x_{m, i}$. The contextual data so drawn are represented as $\\mathcal{C}_m = (x_{m, 1}, y_{m, 1}, \\ldots, x_{m, N}, y_{m, N})$. Finally, draw a query associated with $\\mathcal{C}_m, x_{m, N+1}$, and the model is to predict $y_{m, N+1} $ given $x_{m, N+1}$ and $\\mathcal{C}_m$. In the following experiments set $\\mu$ as a vector of all values equal to one: $\\mu = (1, \\ldots, 1)^T$. In all experiments, consider the number of pairs in the context as $N = 5, \\ldots, 20$ (examine performance of the methods over this range of $N$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "\n",
    "# def generate_linear_batch(\n",
    "#     M, # number of contexts\n",
    "#     N, # samples per context\n",
    "#     dim_output,\n",
    "#     dim_input,\n",
    "#     slope_mean,\n",
    "#     slope_cov,\n",
    "#     intercept_range,\n",
    "#     input_range,\n",
    "#     use_bias=True,\n",
    "# ):\n",
    "#     slope = np.random.multivariate_normal(slope_mean, slope_cov, M + 1)\n",
    "#     intercept = (\n",
    "#         np.random.uniform(intercept_range[0], intercept_range[1], [M + 1])\n",
    "#         if use_bias\n",
    "#         else np.zeros(M + 1)\n",
    "#     )\n",
    "#     outputs = np.zeros([M + 1, N, dim_output])\n",
    "#     init_inputs = np.zeros([M + 1, N, dim_input])\n",
    "#     for func in range(M + 1):\n",
    "#         init_inputs[func] = np.random.uniform(input_range[0], input_range[1], [N, 1])\n",
    "#         outputs[func] = slope[func] * init_inputs[func] + intercept[func]\n",
    "\n",
    "#     C_m = (init_inputs[:-1], outputs[:-1])\n",
    "#     query = (init_inputs[-1], outputs[-1])\n",
    "#     return C_m, query\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "def generate_linear_batch(\n",
    "    M,\n",
    "    N,\n",
    "    dim_output=1,\n",
    "    dim_input=10,\n",
    "    use_bias=False,\n",
    "    bias_sigma=0.1,\n",
    "):\n",
    "    # Draw the weights from a multivariate normal distribution\n",
    "    slope = np.random.multivariate_normal(np.ones(dim_input), np.eye(dim_input), M + 1)\n",
    "\n",
    "    intercept = np.random.normal(0, bias_sigma, M + 1) if use_bias else np.zeros(M + 1)\n",
    "\n",
    "    outputs = np.zeros([M + 1, N, dim_output])\n",
    "    init_inputs = np.zeros([M + 1, N, dim_input])\n",
    "\n",
    "    for func in range(M + 1):\n",
    "        # Draw the inputs from a multivariate normal distribution\n",
    "        init_inputs[func] = np.random.multivariate_normal(\n",
    "            np.zeros(dim_input), np.eye(dim_input), N\n",
    "        )\n",
    "\n",
    "        outputs[func] = (\n",
    "            np.dot(slope[func], init_inputs[func].T).reshape(-1, 1) + intercept[func]\n",
    "        )\n",
    "\n",
    "    C_m = (torch.tensor(init_inputs[:-1]), torch.tensor(outputs[:-1]))\n",
    "    query = (torch.tensor(init_inputs[-1]), torch.tensor(outputs[-1]))\n",
    "    return C_m, query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 10\n",
    "N = 20\n",
    "dim_output = 1\n",
    "dim_input = 10\n",
    "use_bias = False\n",
    "bias_sigma = 0.1\n",
    "\n",
    "C_m, query = generate_linear_batch(\n",
    "    M=M,\n",
    "    N=N,\n",
    "    dim_output=dim_output,\n",
    "    dim_input=dim_input,\n",
    "    use_bias=use_bias,\n",
    "    bias_sigma=bias_sigma,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float64\n",
      "torch.float64\n",
      "torch.float64\n",
      "torch.float64\n",
      "torch.float64\n",
      "torch.float64\n",
      "torch.float64\n",
      "torch.float64\n",
      "torch.float64\n",
      "torch.float64\n"
     ]
    }
   ],
   "source": [
    "inputa, labela = C_m\n",
    "inputb, labelb = query\n",
    "# print(inputb.shape, labelb.shape)\n",
    "for task in range(inputa.size(0)):\n",
    "    print(labela[task].dtype)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Perform in-context learning based on MAML, and assume a linear model $f_w(x) = w^T x$, with model parameters $w$. Use MAML to learn good initialization parameters $w_0$, based on $\\mathcal{C}_m = (x_{m, 1}, y_{m, 1}, \\ldots, x_{m, N}, y_{m, N})$ for $m = 1, \\ldots, M$, then apply the model to predict $y_{M+1, N+1}$ for $\\mathcal{C}_{M+1} = (x_{M+1, 1}, y_{M+1, 1}, \\ldots, x_{M+1, N}, y_{M+1, N}, y_{M+1, N+1})$. Show results for various sizes of $M$ and $N$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "from torch.nn import MSELoss\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def train(model, M, N, mu, num_epochs, learning_rate):\n",
    "    optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "    loss_fn = MSELoss()\n",
    "\n",
    "    pbar = tqdm(range(num_epochs), desc=\"Training ...\")\n",
    "    min_loss = float(\"inf\")\n",
    "\n",
    "    for epoch in pbar:  # Iterate over pbar\n",
    "        for _ in range(M):\n",
    "            # Generate context\n",
    "            C_m, query = generate_linear_batch(\n",
    "                M=M,\n",
    "                N=N,\n",
    "            )\n",
    "\n",
    "            # Data is already in PyTorch tensors\n",
    "            inputa, labela = C_m\n",
    "\n",
    "            # Generate query\n",
    "            inputb, labelb = query\n",
    "\n",
    "            # Forward pass\n",
    "            outputas, lossesb = model.task_metalearn(\n",
    "                inputa.float(),\n",
    "                inputb.float(),\n",
    "                labela.float(),\n",
    "                labelb.float(),\n",
    "            )\n",
    "\n",
    "            # Compute loss\n",
    "            loss = sum(lossesb)\n",
    "            totalloss += loss.item()\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        avg_loss = totalloss / M\n",
    "\n",
    "        # Print loss every 100 epochs\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {avg_loss:.2f}\")\n",
    "\n",
    "        # Save the model if the loss decreased\n",
    "        if avg_loss < min_loss:\n",
    "            min_loss = avg_loss\n",
    "            torch.save(model.state_dict(), f\"models/MAML_M{M}_N{N}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from maml_pytorch import MAMLModel\n",
    "\n",
    "lr = 0.001\n",
    "dim_input = 10\n",
    "model = MAMLModel(\n",
    "    dim_input=dim_input,\n",
    "    dim_hidden=[40, 40],\n",
    "    dim_output=1,\n",
    "    num_updates=5,\n",
    "    learning_rate=lr,\n",
    ")\n",
    "model.float()\n",
    "mu = np.ones(dim_input)\n",
    "# train(model, M=10, N=20, mu=mu, num_epochs=500, learning_rate=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Add noise to the observed $y_{m, i}$ , where now it is $y_{m, i} = w_m^T x_{m, i} + \\epsilon_{m, i}$, where $\\epsilon_{m, i} \\sim \\mathcal{N}(0, \\sigma^2)$. Examine different sizes of $\\sigma^2$ and examine the robustness of MAML to such noise-induced model mismatch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) For (a) above, rather than doing MAML, use a Transformer with linear attention. As in https://arxiv.org/abs/2212.07677, show results as a function of $M$ and $N$, and compare your results to MAML, and also to ordinary least squares (OLS). Do the linear-attention Transformer two ways: \n",
    "\n",
    "1. based on the *designed* Transformer parameters \n",
    "\n",
    "2.  also when the Transformer parameters are learned, based on ${\\mathcal{C}_m}_{m=1, M}$ \n",
    "\n",
    "Compare the results of the Transformer with designed and learned parameters. There are similar experiments in the paper, that you should design your results on. Consider performance with and without noise $\\epsilon_{m,i}$ added to the observed outcomes, like in (b)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now consider contextual data $\\mathcal{C}_m = (x_{m, 1}, y_{m, 1}, \\ldots, x_{m, N}, y_{m, N})$ where in each case $y_{m, i} = f_{w_m}(x_{m, i}) = w_m^T x_{m,i}$, where each $w_m \\sim \\mathcal{N}(\\mathbf{0}_d, I_d)$, where $d = 10$. This is as above, but now the manner with which $x_{m,i}$ are drawn is different: Consider two 10-dimensional real-valued vectors: $v=(v_1, \\ldots, v_{10})^T$ and $u=(u_1, \\ldots, u_{10})^T$, where $v_j = cos(\\frac{j \\pi}{5})$ and $u_j = sin(\\frac{j \\pi}{5})$, for $j = 1, \\ldots, 10$. Each $x_{m,i} = \\alpha v + \\beta u + \\epsilon$, where $\\alpha \\sim \\mathcal{N}(0, 1)$, $\\beta \\sim \\mathcal{N}(0, 1)$, and $\\epsilon = (\\epsilon_1, \\ldots, \\epsilon_{10})^T$ , with $\\epsilon_j \\sim \\mathcal{N}(0, \\frac{1}{100})$. Note, what this says is that each $x_{m,i} \\in \\mathbb{R}^{10} $ is expressed as a randomly scaled sum of two factors, with the two factors represented by $u$ and $v$ (with respective random weights $\\alpha$ and $\\beta$), and the vector $\\epsilon$ represents a small amount of additive noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) What is the covariance of the data $x\\in \\mathbb{R}^{10}$ drawn in the manner specified above?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e) For data generated as above, use the designed Transformer with linear attention to perform few-shot learning on new contextual data $\\mathcal{C}_{M+1} = (x_{m+1,1}, y_{M+1,1}, \\ldots , x_{M+1, N}, y_{M+1, N}, x_{M+1, N+1})$, and compare that to the performance of a linear Transformer trained on ${\\mathcal{C}_m}_{m=1,M}$ (in the latter, you *learn* the\n",
    "Transformer parameters via ${\\mathcal{C}_m}_{m=1,M} $ ). Consider performance for various settings of $M$, for the trained model. For this problem, pay special attention to the GD++ framework in https://arxiv.org/abs/2212.07677; we will also discuss this in class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f) Repeat the experiments in (c)-(d) using softmax attention and the full Transformer, as in https://arxiv.org/abs/2208.01066; in this case we do not make the linear-attention assumption and follow the framework in the referenced paper (and that we discussed in class). Compare the performance of the learned Transformer with softmax attention to what you got in (e) with linear attention. Which method works better, in the sense of model accuracy as a function of Transformer depth?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
